{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026faf60-6706-4081-8efb-155b09f47e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Unified Telugu News Scheduler Pipeline (UPDATED FOR YOUR REQUIREMENTS)\n",
    "\n",
    "- Asynchronously crawls Telugu news websites (crawl4ai)\n",
    "- Extracts & cleans articles\n",
    "- Filters to mostly-Telugu content (drops mojibake / non-Telugu)\n",
    "- Chunks text with overlap\n",
    "- Deduplicates at article level (url + md5(content))\n",
    "- Deduplicates at chunk level (url + md5(chunk))\n",
    "- Creates Cohere embeddings ONLY for new chunks\n",
    "- Appends new embeddings to CSV (JSON-encoded)\n",
    "- Rebuilds FAISS index + metadata CSV with ID column\n",
    "- Scheduler runs at fixed interval\n",
    "\n",
    "FILES USED:\n",
    "- all_telugu_news_articles.csv\n",
    "- all_telugu_chunk_embeddings.csv\n",
    "- all_telugu_faiss_metadata.csv\n",
    "- telugu_faiss_index.index\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import hashlib\n",
    "import asyncio\n",
    "import logging\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import nest_asyncio\n",
    "import schedule\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "import csv\n",
    "csv.field_size_limit(10_000_000)\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "try:\n",
    "    import cohere\n",
    "except Exception:\n",
    "    cohere = None\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ---------- Logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"telugu_pipeline\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "COHERE_API_KEY = \"DSdAuREU39x4mYDJaSDZ3DEmGM1x8000F7BZuRf2\"\n",
    "\n",
    "ARTICLES_CSV = \"all_telugu_news_articles.csv\"\n",
    "CHUNKS_CSV = \"all_telugu_chunk_embeddings_clean.csv\"\n",
    "META_CSV = \"all_telugu_faiss_metadata.csv\"\n",
    "FAISS_INDEX_FILE = \"telugu_faiss_index.index\"\n",
    "EMBEDDINGS_NPY = \"embeddings_array.npy\"\n",
    "\n",
    "BATCH_SIZE = 32                     # Safer for trial\n",
    "EMBED_MODEL = \"embed-multilingual-v3.0\"\n",
    "EMBED_INPUT_TYPE = \"search_document\"\n",
    "EMBED_DIM = 1024                     # Cohere multilingual dim\n",
    "\n",
    "CATEGORY_START_URLS = [\n",
    "    \"https://www.eenadu.net/\",\n",
    "    \"https://www.eenadu.net/andhra-pradesh\",\n",
    "    \"https://www.eenadu.net/telangana\",\n",
    "    \"https://www.eenadu.net/india\",\n",
    "    \"https://www.eenadu.net/world\",\n",
    "    \"https://www.eenadu.net/business\",\n",
    "    \"https://www.eenadu.net/sports\",\n",
    "    \"https://www.eenadu.net/pratibha\",\n",
    "    \"https://www.eenadu.net/movies\",\n",
    "    \"https://www.eenadu.net/youth\",\n",
    "    \"https://www.eenadu.net/technology\",\n",
    "    \"https://www.eenadu.net/health\",\n",
    "    \"https://www.eenadu.net/latest-news\",\n",
    "    \"https://www.eenadu.net/trending-news\",\n",
    "    \"https://www.eenadu.net/web-stories\",\n",
    "    \"https://www.eenadu.net/explained\",\n",
    "    \"https://www.eenadu.net/sunday-magazine\",\n",
    "    \"https://epaper.andhrajyothy.com/index.aspx\",\n",
    "    \"https://epaper.sakshi.com/index.aspx\",\n",
    "    \"https://www.andhrajyothy.com/\",\n",
    "    \"https://www.sakshi.com/\",\n",
    "    \"https://www.manabadi.co.in/\",\n",
    "]\n",
    "\n",
    "TARGET_PATTERNS = [\n",
    "    \"/telugu-news/\", \"/breaking-news/\", \"/business/\", \"/sports/\", \"/movies/\", \"/districts/\",\n",
    "    \"/andhra-pradesh/\", \"/telangana/\", \"/india/\", \"/world/\", \"/youth/\", \"/technology/\",\n",
    "    \"/health/\", \"/devotional/\", \"/real-estate/\", \"/web-stories/\", \"/article/\", \"/district/\",\n",
    "    \"/news/\", \"/state/\", \"/job\", \"/jobs\", \"/career\", \"/careers\", \"/notification\", \"/advt\",\n",
    "    \"/education\", \"/result\", \"/exam\"\n",
    "]\n",
    "\n",
    "# ---------- Initialize Cohere ----------\n",
    "if cohere is None:\n",
    "    logger.error(\"cohere package not installed. Run: pip install cohere\")\n",
    "    co_client = None\n",
    "else:\n",
    "    try:\n",
    "        co_client = cohere.Client(COHERE_API_KEY)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to init Cohere client: {e}\")\n",
    "        co_client = None\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def safe_read_csv(path: str, **kwargs) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", engine=\"python\", **kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        logger.warning(f\"UTF-8 decode failed for {path}, retrying with latin1\")\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {path} with latin1: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def write_csv_safe(df: pd.DataFrame, path: str, mode: str = \"w\"):\n",
    "    try:\n",
    "        df.to_csv(path, mode=mode, header=(mode == \"w\"), index=False, encoding=\"utf-8\")\n",
    "        logger.info(f\"Saved {len(df)} rows to {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write {path}: {e}\")\n",
    "\n",
    "def md5_hash_text(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def looks_like_telugu(text: str, min_ratio: float = 0.3) -> bool:\n",
    "    \"\"\"Return True if >= min_ratio of chars are in Telugu Unicode block.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return False\n",
    "    total = len(text)\n",
    "    telugu_chars = sum(1 for ch in text if \"\\u0C00\" <= ch <= \"\\u0C7F\")\n",
    "    return telugu_chars / max(total, 1) >= min_ratio\n",
    "\n",
    "def clean_text_for_chunking(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    s = re.sub(r\"(?i)(privacy policy|read more|advertisement|subscribe|follow us).*\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def safe_get_text(el):\n",
    "    try:\n",
    "        return el.get_text(strip=True) if el else \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def detect_source(url: str) -> str:\n",
    "    u = url.lower()\n",
    "    if \"eenadu.net\" in u:\n",
    "        return \"eenadu\"\n",
    "    if \"andhrajyothy\" in u:\n",
    "        return \"andhrajyothy\"\n",
    "    if \"sakshi.com\" in u:\n",
    "        return \"sakshi\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_date_from_soup(soup: BeautifulSoup, url: str) -> str:\n",
    "    for tag in [\"time\", \"span\", \"div\"]:\n",
    "        for el in soup.find_all(tag):\n",
    "            txt = safe_get_text(el)\n",
    "            if not txt:\n",
    "                continue\n",
    "            m = re.search(r\"(\\d{4}-\\d{2}-\\d{2})\", txt)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "            m2 = re.search(r\"(\\d{2})/(\\d{2})/(\\d{4})\", txt)\n",
    "            if m2:\n",
    "                d, m_, y = m2.groups()\n",
    "                return f\"{y}-{m_}-{d}\"\n",
    "    m = re.search(r\"/(\\d{4})/(\\d{2})/(\\d{2})\", url)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
    "    return datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def clean_article_content(soup: BeautifulSoup, source: str, url: str):\n",
    "    date = extract_date_from_soup(soup, url)\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "\n",
    "    if source == \"eenadu\":\n",
    "        title = safe_get_text(soup.find(\"h1\")) or safe_get_text(soup.title)\n",
    "        tag = soup.select_one(\"article\") or soup.select_one(\"div.article-content\") \\\n",
    "              or soup.select_one(\"div.story-content\")\n",
    "        if tag:\n",
    "            content = tag.get_text(separator=\"\\n\", strip=True)\n",
    "    elif source == \"andhrajyothy\":\n",
    "        title = safe_get_text(soup.find(\"h3\")) or safe_get_text(soup.title)\n",
    "        tag = soup.select_one(\"div.article-section\") or soup.select_one(\"div.col-md-10\")\n",
    "        if tag:\n",
    "            content = tag.get_text(separator=\"\\n\", strip=True)\n",
    "    elif source == \"sakshi\":\n",
    "        title = safe_get_text(soup.find(\"h1\")) or safe_get_text(soup.title)\n",
    "        tag = soup.select_one(\"div.news-body\") or soup.select_one(\"div.storyContent\")\n",
    "        if tag:\n",
    "            content = tag.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Fallback\n",
    "    if not content:\n",
    "        paras = [safe_get_text(p) for p in soup.find_all(\"p\") if len(safe_get_text(p)) > 40]\n",
    "        content = \"\\n\".join(paras)\n",
    "\n",
    "    if not title:\n",
    "        title = content[:120] if content else \"Untitled\"\n",
    "\n",
    "    # Telugu check: drop obviously non-Telugu / mojibake\n",
    "    if not looks_like_telugu(content, min_ratio=0.3):\n",
    "        logger.warning(f\"Content for {url} does not look like Telugu; skipping.\")\n",
    "        return \"\", \"\", date\n",
    "\n",
    "    return title.strip(), content.strip(), date\n",
    "\n",
    "# ---------- Chunker ----------\n",
    "\n",
    "def recursive_text_splitter(text: str, max_chunk_size: int = 350,min_chunk_size: int = 100, overlap: int = 80) -> List[str]:\n",
    "    text = clean_text_for_chunking(text)\n",
    "    if len(text) <= max_chunk_size:\n",
    "        return [text] if len(text) >= min_chunk_size and looks_like_telugu(text) else []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = min(start + max_chunk_size, L)\n",
    "        split_point = end\n",
    "        for sep in [\"\\n\", \". \", \"! \", \"? \"]:\n",
    "            pos = text.rfind(sep, start, end)\n",
    "            if pos != -1 and (pos - start) >= min_chunk_size:\n",
    "                split_point = pos + len(sep)\n",
    "                break\n",
    "        chunk = text[start:split_point].strip()\n",
    "        if len(chunk) >= min_chunk_size and looks_like_telugu(chunk):\n",
    "            chunks.append(chunk)\n",
    "        next_start = split_point - overlap\n",
    "        if next_start <= start:\n",
    "            next_start = split_point\n",
    "        start = next_start\n",
    "\n",
    "    cleaned = []\n",
    "    seen_hashes = set()\n",
    "    for c in chunks:\n",
    "        ch = re.sub(r\"\\s+\", \" \", c).strip()\n",
    "        if len(ch) < min_chunk_size:\n",
    "            continue\n",
    "        h = md5_hash_text(ch)\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "        cleaned.append(ch)\n",
    "    return cleaned\n",
    "\n",
    "# ---------- Dedup helpers ----------\n",
    "\n",
    "def load_existing_articles_index(path: str = ARTICLES_CSV) -> Dict[str, str]:\n",
    "    df = safe_read_csv(path)\n",
    "    mapping = {}\n",
    "    if df.empty:\n",
    "        return mapping\n",
    "    for _, row in df.iterrows():\n",
    "        url = str(row.get(\"url\", \"\")).strip()\n",
    "        content = str(row.get(\"content\", \"\"))\n",
    "        mapping[url] = md5_hash_text(content)\n",
    "    return mapping\n",
    "\n",
    "def load_existing_chunk_keys(path: str = CHUNKS_CSV) -> set:\n",
    "    df = safe_read_csv(path)\n",
    "    if df.empty:\n",
    "        return set()\n",
    "    keys = set()\n",
    "    for _, row in df.iterrows():\n",
    "        url = str(row.get(\"url\", \"\")).strip()\n",
    "        chunk = str(row.get(\"chunk\", \"\")).strip()\n",
    "        ch = md5_hash_text(chunk)\n",
    "        keys.add((url, ch))\n",
    "    return keys\n",
    "\n",
    "# ---------- Crawling ----------\n",
    "\n",
    "async def collect_candidate_links(max_depth=5, max_pages_per_site=50) -> List[str]:\n",
    "    links = set()\n",
    "    try:\n",
    "        async with AsyncWebCrawler(max_depth=max_depth, max_pages=max_pages_per_site) as crawler:\n",
    "            for seed in CATEGORY_START_URLS:\n",
    "                try:\n",
    "                    result = await crawler.arun(seed)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Crawler failed for seed {seed}: {e}\")\n",
    "                    continue\n",
    "                soup = BeautifulSoup(result.html, \"html.parser\", from_encoding=\"utf-8\")\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"].strip()\n",
    "                    if href.startswith(\"//\"):\n",
    "                        href = \"https:\" + href\n",
    "                    if href.startswith(\"/\"):\n",
    "                        parts = seed.split(\"/\")\n",
    "                        if len(parts) >= 3:\n",
    "                            href = parts[0] + \"//\" + parts[2] + href\n",
    "                    if any(pat in href for pat in TARGET_PATTERNS):\n",
    "                        norm = href.split(\"#\")[0].split(\"?\")[0]\n",
    "                        links.add(norm)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error collecting links: {e}\")\n",
    "    return list(links)\n",
    "\n",
    "async def scrape_articles_from_links(links: List[str]) -> List[dict]:\n",
    "    articles = []\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        for i, url in enumerate(links):\n",
    "            try:\n",
    "                result = await crawler.arun(url)\n",
    "                soup = BeautifulSoup(result.html, \"html.parser\", from_encoding=\"utf-8\")\n",
    "                source = detect_source(url)\n",
    "                title, content, date = clean_article_content(soup, source, url)\n",
    "                if content and len(content) > 80:\n",
    "                    articles.append({\"url\": url, \"title\": title, \"content\": content, \"date\": date})\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to fetch {url}: {e}\")\n",
    "    return articles\n",
    "\n",
    "# ---------- Embedding ----------\n",
    "\n",
    "def embed_text_batch(texts: List[str]) -> List[List[float]]:\n",
    "    if co_client is None:\n",
    "        raise RuntimeError(\"Cohere client not initialized.\")\n",
    "    out_embeddings = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i + BATCH_SIZE]\n",
    "        try:\n",
    "            resp = co_client.embed(texts=batch, model=EMBED_MODEL, input_type=EMBED_INPUT_TYPE)\n",
    "            for vec in resp.embeddings:\n",
    "                if len(vec) != EMBED_DIM:\n",
    "                    raise ValueError(f\"Unexpected embedding dim {len(vec)}\")\n",
    "            out_embeddings.extend([list(vec) for vec in resp.embeddings])\n",
    "            logger.info(f\"Embedded batch {i // BATCH_SIZE + 1} ({len(batch)} items)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cohere embed error for batch starting at {i}: {e}\")\n",
    "            out_embeddings.extend([None] * len(batch))\n",
    "        time.sleep(0.25)\n",
    "    return out_embeddings\n",
    "\n",
    "# ---------- FAISS Rebuild ----------\n",
    "def rebuild_faiss_from_csv(chunks_csv_path: str = CHUNKS_CSV, meta_csv_path: str = META_CSV, faiss_index_path: str = FAISS_INDEX_FILE):\n",
    "    if faiss is None:\n",
    "        logger.warning(\"faiss not available; skipping FAISS rebuild.\")\n",
    "        return\n",
    "\n",
    "    df = safe_read_csv(chunks_csv_path)\n",
    "    if df.empty or \"embedding\" not in df.columns:\n",
    "        logger.warning(\"No chunk embeddings found; skipping FAISS rebuild.\")\n",
    "        return\n",
    "\n",
    "    def parse_embedding_field(x):\n",
    "        try:\n",
    "            # embeddings are saved as JSON strings\n",
    "            arr = json.loads(str(x))\n",
    "        except Exception:\n",
    "            try:\n",
    "                arr = ast.literal_eval(str(x))\n",
    "            except Exception:\n",
    "                return None\n",
    "        if isinstance(arr, list) and len(arr) == EMBED_DIM:\n",
    "            return np.array(arr, dtype=\"float32\")\n",
    "        return None\n",
    "\n",
    "    df[\"embedding_obj\"] = df[\"embedding\"].apply(parse_embedding_field)\n",
    "    df = df[df[\"embedding_obj\"].notnull()].reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        logger.warning(\"No valid embeddings after parsing; skipping FAISS rebuild.\")\n",
    "        return\n",
    "\n",
    "    # Add ID column for FAISS metadata alignment\n",
    "    df.insert(0, \"id\", range(len(df)))\n",
    "\n",
    "    meta_cols = [\"id\", \"url\", \"title\", \"content\", \"date\", \"chunk\", \"geographical\", \"sector\"]\n",
    "    meta_cols = [c for c in meta_cols if c in df.columns]\n",
    "    df[meta_cols].to_csv(meta_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    embeddings = np.vstack(df[\"embedding_obj\"].values)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    np.save(EMBEDDINGS_NPY, embeddings)\n",
    "\n",
    "    logger.info(f\"✅ Rebuilt FAISS index with {len(df)} vectors (dim {dim}) -> {faiss_index_path}\")\n",
    "    logger.info(f\"✅ Metadata saved to {meta_csv_path} with id column\")\n",
    "\n",
    "\n",
    "# ---------- Chunk + Embed with Dedup ----------\n",
    "\n",
    "def chunk_and_embed_and_save(new_articles: List[dict]):\n",
    "    if not new_articles:\n",
    "        logger.info(\"No new articles to process.\")\n",
    "        return\n",
    "\n",
    "    existing_article_map = load_existing_articles_index(ARTICLES_CSV)\n",
    "    existing_chunk_keys = load_existing_chunk_keys(CHUNKS_CSV)\n",
    "\n",
    "    deduped_articles: Dict[str, dict] = {}\n",
    "    for art in new_articles:\n",
    "        url = str(art.get(\"url\", \"\")).strip()\n",
    "        content = str(art.get(\"content\", \"\"))\n",
    "        h = md5_hash_text(content)\n",
    "        if url in existing_article_map and existing_article_map[url] == h:\n",
    "            continue\n",
    "        deduped_articles[url] = {**art, \"_content_hash\": h}\n",
    "\n",
    "    if not deduped_articles:\n",
    "        logger.info(\"No new or changed articles after deduplication.\")\n",
    "        return\n",
    "\n",
    "    # Save new/updated articles\n",
    "    rows_to_append = []\n",
    "    for url, art in deduped_articles.items():\n",
    "        rows_to_append.append({\n",
    "            \"url\": url,\n",
    "            \"title\": art.get(\"title\", \"\"),\n",
    "            \"content\": art.get(\"content\", \"\"),\n",
    "            \"date\": art.get(\"date\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "        })\n",
    "    df_new_articles = pd.DataFrame(rows_to_append)\n",
    "    if os.path.exists(ARTICLES_CSV):\n",
    "        df_new_articles.to_csv(ARTICLES_CSV, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
    "    else:\n",
    "        df_new_articles.to_csv(ARTICLES_CSV, mode=\"w\", header=True, index=False, encoding=\"utf-8\")\n",
    "    logger.info(f\"✅ Saved {len(df_new_articles)} new/updated articles to {ARTICLES_CSV}\")\n",
    "\n",
    "    # Chunk & embed only new deduped articles\n",
    "    chunk_rows = []\n",
    "    for url, art in deduped_articles.items():\n",
    "        title = art.get(\"title\", \"\")\n",
    "        content = art.get(\"content\", \"\")\n",
    "        date = art.get(\"date\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "        chunks = recursive_text_splitter(content)\n",
    "        for ch in chunks:\n",
    "            ch_key = (url, md5_hash_text(ch))\n",
    "            if ch_key in existing_chunk_keys:\n",
    "                continue\n",
    "            chunk_rows.append({\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"date\": date,\n",
    "                \"chunk\": ch,\n",
    "                \"geographical\": \"unknown\",\n",
    "                \"sector\": \"general\",\n",
    "                \"embedding\": None\n",
    "            })\n",
    "\n",
    "    if not chunk_rows:\n",
    "        logger.info(\"No new chunks to embed.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Preparing to embed {len(chunk_rows)} new chunks.\")\n",
    "\n",
    "    texts = [r[\"chunk\"] for r in chunk_rows]\n",
    "    embeddings = embed_text_batch(texts)\n",
    "\n",
    "    final_rows = []\n",
    "    for idx, row in enumerate(chunk_rows):\n",
    "        emb = embeddings[idx] if idx < len(embeddings) else None\n",
    "        if not emb:\n",
    "            continue\n",
    "        try:\n",
    "            emb_list = [float(x) for x in emb]\n",
    "        except Exception:\n",
    "            logger.exception(\"Embedding parse error; skipping.\")\n",
    "            continue\n",
    "        row[\"embedding\"] = emb_list\n",
    "        final_rows.append(row)\n",
    "\n",
    "    if not final_rows:\n",
    "        logger.info(\"No embeddings succeeded; nothing to save.\")\n",
    "        return\n",
    "\n",
    "    df_chunks = pd.DataFrame(final_rows)\n",
    "    df_chunks[\"embedding\"] = df_chunks[\"embedding\"].apply(json.dumps)\n",
    "    if os.path.exists(CHUNKS_CSV):\n",
    "        df_chunks.to_csv(CHUNKS_CSV, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
    "    else:\n",
    "        df_chunks.to_csv(CHUNKS_CSV, mode=\"w\", header=True, index=False, encoding=\"utf-8\")\n",
    "    logger.info(f\"✅ Saved {len(df_chunks)} new chunk embeddings to {CHUNKS_CSV}\")\n",
    "\n",
    "# ---------- Full pipeline job ----------\n",
    "\n",
    "def pipeline_job(max_depth=3, max_pages_per_site=50):\n",
    "    logger.info(\"\\n--- Scheduler run: starting pipeline ---\")\n",
    "    try:\n",
    "        links = asyncio.run(collect_candidate_links(max_depth=max_depth, max_pages_per_site=max_pages_per_site))\n",
    "        if not links:\n",
    "            logger.info(\"No candidate links found.\")\n",
    "            return\n",
    "        logger.info(f\"Collected {len(links)} candidate links.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to collect links: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        articles = asyncio.run(scrape_articles_from_links(links))\n",
    "        if not articles:\n",
    "            logger.info(\"No articles scraped this run.\")\n",
    "        else:\n",
    "            logger.info(f\"Scraped {len(articles)} articles.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to scrape articles: {e}\")\n",
    "        articles = []\n",
    "\n",
    "    try:\n",
    "        chunk_and_embed_and_save(articles)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during chunk/embed/save: {e}\")\n",
    "\n",
    "    try:\n",
    "        rebuild_faiss_from_csv()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding FAISS: {e}\")\n",
    "\n",
    "    logger.info(\"✅ Pipeline run complete!\\n\")\n",
    "\n",
    "# ---------- Scheduler ----------\n",
    "\n",
    "def start_scheduler(interval_minutes: int = 5, max_depth=3, max_pages_per_site=10):\n",
    "    schedule.clear()\n",
    "    schedule.every(interval_minutes).minutes.do(\n",
    "        lambda: pipeline_job(max_depth=max_depth, max_pages_per_site=max_pages_per_site)\n",
    "    )\n",
    "    logger.info(f\"Scheduler started. Pipeline runs every {interval_minutes} minutes.\")\n",
    "    try:\n",
    "        pipeline_job(max_depth=max_depth, max_pages_per_site=max_pages_per_site)  # run once immediately\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Scheduler stopped by user.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scheduler loop error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_scheduler(interval_minutes=5, max_depth=3, max_pages_per_site=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
